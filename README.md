Enhanced Ship Instance Segmentation in SAR and Optical Images via MobileViT-v3 Fusion Network

Ship instance segmentation is pivotal for various maritime applications, yet it remains challenging due to the complex ocean environment and diverse ship characteristics.  This paper introduces MDFNet, a novel single-stage instance segmentation network based on MobileViT-v3, which effectively integrates convolutional and Transformer mechanisms.  The backbone of MDFNet incorporates the CpnMvitbv3 module, leveraging MobileViT-v3 to enhance the understanding of spatial relationships between objects through global attention.  The neck network employs DODConv with dynamic weights to handle complex backgrounds and multi-scale objects.  Furthermore, we introduce Focal-SIoU loss to improve small object detection, manage challenging samples, and optimize boundary precision.  Experimental results on the SSDD (SAR) and MarShipInsSeg (visible light) datasets demonstrate that MDFNet achieves state-of-the-art performance with an average precision of 63.8% on SSDD and 49.4% on MarShipInsSeg, surpassing existing methods by significant margins.  Notably, MDFNet maintains a compact model size of 24MB, showcasing its efficiency and adaptability across different datasets and environments.

We will introduce the three core innovative points of this code：
  1.The initialization of CpnMvitbv3 includes dimensionality reduction of the input features (via convolution), followed by feature enhancement using the MViTBv3 module. Its input channel c_in is adjusted to the output channel c_out, with intermediate channel separation and merging, making the data transformation more efficient.
MViTBv3 (see Figure 3B), as the core submodule, combines standard convolution layers, 1x1 convolution layers, and Transformer mechanisms. The output features are further refined through the MV3Block. Initially, MViTBv3 uses regular convolution layers and 1x1 convolution layers to perform dimensional transformations and fusion of the input features. It then further enhances the features by combining 1x1 convolution layers with regular convolution layers and MBTransformer. Several skip connections are retained within MViTBv3 to alleviate the vanishing gradient problem and enhance learning capabilities. Finally, inspired by MobileViT-V3, pointwise convolutions and depthwise separable convolutions are employed to improve computational efficiency, with the output features being further refined by the MV3Block.For MBTransformer (see Figure 4A), it is a module based on the Transformer architecture, primarily used for feature extraction or transformation in deep learning models. This module comprises two submodules, each incorporating a (PreNorm + Attention) block and a (PreNorm + FeedForward) block. PreNorm first normalizes the input, then processes it through either the Attention or FeedForward module to ensure more stable gradient propagation. The Attention module utilizes a multi-head attention mechanism to grab correlations between inputs and generate context-aware representations, while the FeedForward module, a multi-layer perceptron (MLP), further extracts features and enhances nonlinear expressiveness, followed by a residual connection.For Mv3Block (see Figure 4B), this module achieves efficient feature extraction while focusing on lightweight and computational efficiency. First, pointwise convolutions are used for channel expansion, followed by depthwise separable convolutions, and finally, pointwise convolutions are applied again to restore the channel number. Pointwise convolutions change the number of channels using 1x1 convolutions, while depthwise separable convolutions convolve each channel separately, significantly reducing computation. When the stride is 1 and the input and output channels are the equal, a residual connection is applied.
  2.We replaced two standard convolutional layers in the neck network with DODConv layers (see Figure 2A), inspired by DOConv . The DODConv module consists of a DOConv2d layer, a BatchNorm regularization layer, and a SiLU activation function. The DOConv2d layer includes two weight matrices: a base weight matrix, initialized with Kaiming random initialization for the kernel's foundational weights, and a dynamic weight matrix, which requires the addition of a diagonal matrix d_diag to ensure favorable properties of the dynamic weights during the initialization phase. The interaction between the base weight matrix and the dynamic weight matrix produces the final dynamic weights used for the actual convolution operations, followed by the convolution computation. In the figure, the B W Matrix and D W Matrix represent the base weight matrix and the dynamic weight matrix, respectively, while D Weight refers to the dynamic weight obtained through the interaction of the two.
  3.This paper presents a novel combined loss function, Focal-SIoU loss, which substitutes the traditional IoU loss function in bounding box regression. Focal-SIoU is an enhanced optimization loss function that leverages the benefits of Focal Loss . for addressing class imbalance and SIoU . for improving bounding box regression accuracy. It is designed to enhance bounding box localization accuracy while helping the model concentrate on challenging examples during detection tasks. This loss function is especially useful in object detection and instance segmentation, particularly when handling severe class imbalance and intricate backgrounds.

Environment dependencies and core packages: windows11 system, pytorch=1.12.0, python=3.8,numpy=1.23.2,mmcv=1.6.2,timm=0.6.7, opencv-python=4.10.0.84，Run python train_seg.py --cfg ultralytics\cfg\models\cfg2024\YOLOv8-Seg\yyyy.yaml directly on the terminal to start training，The hyperparameters can be set using our default settings. If you want to use other hyperparameters, please modify them in the train_deg.py and ultralytics/cfg/defaulted.yaml files

Our dataset has been shared on Baidu Cloud：https://pan.baidu.com/s/1w5RyxfPQbvZNPUNXjOLztw ，Extracted code: is3a,

If you feel that the code is helpful to you, please cite our paper：“Enhanced Ship Instance Segmentation in SAR and Optical Images via MobileViT-v3 Fusion Network”Expected to be published in The Visual Computer journal

If you have any questions, please contact us at：m230200742@st.shou.edu.cn;

